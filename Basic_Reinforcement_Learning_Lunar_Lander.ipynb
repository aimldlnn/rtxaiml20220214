{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Basic Reinforcement Learning - Lunar Lander.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aimldlnn/rtxaiml20220214/blob/main/Basic_Reinforcement_Learning_Lunar_Lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiBbhnu85woU"
      },
      "source": [
        "# Basic Reinforcement Learning\n",
        "\n",
        "This example shows how a Lunar Lander can be trained to learn how to land between the flags on the ever-changing moon surface.  This example is a more \"verbose\" example, as you can see the classes for Neural Net predictors and for RL Support that I have created.  These classes have been generalized into other applications, including my Raspberry Pi Rover.\n",
        "\n",
        "**Updates**: Have figured out a more robust way to install gym.  Previous approach stopped working (actually during a class!) after a CoLab update.\n",
        "\n",
        "**Author**: W. Tod Newman\n",
        "##Learning Objectives\n",
        "\n",
        "   *  Learn the concepts behind reinforcement learning\n",
        "   *  Understand how the Deep Q-Learning works at the highest level\n",
        "   *  See the demo on how well a trained model will work (show the art of the possible!)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jwhl_P-U86R"
      },
      "source": [
        "## Needed Libraries\n",
        "\n",
        "These are libraries that will allow this code to run.  They are not installed in CoLaboratory by default so we need to load them.  They will load even through a firewall.\n",
        "\n",
        "Sit back and relax.  This will take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUu2y9UllZpd"
      },
      "source": [
        "%%time\n",
        "!pip install np_utils\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip -q install -Iv pyglet==1.5.0\n",
        "!pip install gym box2d box2d-kengz\n",
        "!pip install statistics\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay\n",
        "!apt-get install python-opengl -y\n",
        "!sudo apt-get update --fix-missing && apt-get -qqq install xvfb -y > /dev/null\n",
        "#!apt install xvfb -y\n",
        "!apt install ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W435O97VXNtg"
      },
      "source": [
        "## GenNet - Tod's RL Generic Neural Net class\n",
        "\n",
        "This class is very useful and generalizes across a number of RL applications.  It works for other Atari games as well as my Raspberry Pi-based autonomous rover.  This support two neural nets, one fully connected network with two hidden layers that uses the ADAM optimizer and one Convolutional Neural network that has 2 hidden convolutional layers and one hidden fully connected layer (plus code commented out to add a third convolutional layer if desired).  The CNN uses the RMSProp Optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P4iGj-gN0QI"
      },
      "source": [
        "import numpy as np\n",
        "import np_utils\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Activation, Dropout, LeakyReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "K.image_data_format()\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "import csv\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "class GenNet(object):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.verbose = kwargs.get('verbose', False)\n",
        "        self.nodes_1 = kwargs.get('nodes_1', 50)\n",
        "        self.nodes_2 = kwargs.get('nodes_2', 50)\n",
        "        self.nodes_3 = kwargs.get('nodes_3', 50)\n",
        "        self.num_actions = kwargs.get('num_actions', 4)\n",
        "        self.num_inputs = kwargs.get('num_inputs', 8)\n",
        "\n",
        "        self.load_weights = kwargs.get('load_weights', False)\n",
        "        self.save_weights = kwargs.get('save_weights', False)\n",
        "\n",
        "        self.log_file = kwargs.get('log_file')\n",
        "        self.weights_file = kwargs.get('weights_file', None)\n",
        "        #self.model_out = kwargs.get('model_file', None)\n",
        "        self.obs_shape = kwargs.get('obs_shape')\n",
        "        self.loss_log = []\n",
        "        self.learning_rate = .0005\n",
        "\n",
        "        self.model = self.nn()\n",
        "        #self.model = self.cnn()\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"\"\"\n",
        "                    Creating neural net with options:\n",
        "                    Nodes, layer 1: %d\n",
        "                    Nodes, layer 2: %d\n",
        "                    Load weights?: %s\n",
        "                    Save weights?: %s\n",
        "                    Weights file: %s\n",
        "                    Log file: %s\n",
        "                \"\"\"\n",
        "                % (self.nodes_1,\n",
        "                   self.nodes_2,\n",
        "                   str(self.load_weights),\n",
        "                   str(self.save_weights),\n",
        "                   self.weights_file,\n",
        "                   #self.model_out,\n",
        "                   self.log_file)\n",
        "            )\n",
        "            \n",
        "    #\n",
        "    #  This is a TF function we create to return a direct inference of the model.\n",
        "    #  We do this because it's much faster when executed in a loop.\n",
        "    #\n",
        "    @tf.function\n",
        "    def serve(self, x):\n",
        "        return self.model(x, training=False)\n",
        "\n",
        "    def cnn(self):\n",
        "        input_shape = (1,84,110)\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(16, 8, strides=(4, 4),\n",
        "                              padding='valid',\n",
        "                              activation='relu',\n",
        "                              input_shape=input_shape,\n",
        "                              data_format='channels_first'\n",
        "                              ))\n",
        "\n",
        "        # Second convolutional layer\n",
        "        model.add(Conv2D(32, 4, strides=(2, 2),\n",
        "                              padding='valid',\n",
        "                              activation='relu',\n",
        "                              input_shape=input_shape,\n",
        "                              data_format='channels_first'\n",
        "                              ))\n",
        "        '''\n",
        "        # Third convolutional layer shown for reference (I don't use it...)\n",
        "        model.add(Conv2D(60, 3, strides=(1, 1),\n",
        "                              padding='valid',\n",
        "                              activation='relu',\n",
        "                              input_shape=input_shape,\n",
        "                              data_format='channels_first'\n",
        "                              ))\n",
        "        '''\n",
        "        # Flatten the convolution output\n",
        "        model.add(Flatten())\n",
        "\n",
        "        # First dense layer\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        \n",
        "        # Output layer\n",
        "        model.add(Dense(self.num_actions))\n",
        "        \n",
        "        # Optimizer - We'll pick Nadam because it works consistently and uses Momentum\n",
        "        optimizer = optimizers.Nadam(lr=self.learning_rate)\n",
        "\n",
        "        # Compile the model and assign loss function and metrics functions as well \n",
        "        # as optimizer.\n",
        "        model.compile(loss='mean_squared_error',\n",
        "                           optimizer=optimizer,\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "        # Load weights if desired                   \n",
        "        if self.load_weights and self.weights_file is not None:\n",
        "            if self.verbose:\n",
        "                print(\"Loading model from %s\" % self.weights_file)\n",
        "            model.load_weights(self.weights_file)\n",
        "\n",
        "        return model\n",
        "        \n",
        "        \n",
        "    def nn(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # First layer.\n",
        "        model.add(Dense(\n",
        "            self.nodes_1, \n",
        "            input_shape=(self.num_inputs,)\n",
        "        ))\n",
        "        model.add(LeakyReLU(alpha=0.005))\n",
        "        # If I could figure it out, we could use a training switch \n",
        "        # to turn dropout on when training and off when predicting.\n",
        "\n",
        "        #model.add(Dropout(0.2))\n",
        "\n",
        "        # Second layer.\n",
        "        model.add(Dense(self.nodes_2))\n",
        "        model.add(LeakyReLU(alpha=0.005))\n",
        "        #model.add(Dropout(0.2))\n",
        "        \n",
        "        # Third layer.\n",
        "        model.add(Dense(self.nodes_3))\n",
        "        model.add(LeakyReLU(alpha=0.005))\n",
        "\n",
        "        # Output layer.\n",
        "        model.add(Dense(self.num_actions))\n",
        "        model.add(Activation('linear'))\n",
        "\n",
        "        # Compile the model and assign loss function and metrics functions as well \n",
        "        # as optimizer.\n",
        "        optimizer = optimizers.Nadam(lr=self.learning_rate)\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer, metrics=['accuracy'])\n",
        "        \n",
        "        # Load weights if desired        \n",
        "        if self.load_weights and self.weights_file is not None:\n",
        "            if self.verbose:\n",
        "                print(\"Loading model from %s\" % self.weights_file)\n",
        "            model.load_weights(self.weights_file)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def predict(self, state):\n",
        "        try:\n",
        "            #return self.model.predict(state.reshape(1, self.num_inputs)) # Slowest method as of TF2.0\n",
        "            #return self.model.predict_on_batch(state.reshape(1, self.num_inputs)) # 10x faster than Predict          \n",
        "            return self.serve(state.reshape(1, self.num_inputs)) # This is 100x faster than Predict\n",
        "            \n",
        "        except:\n",
        "            raise Exception(\"PredictionError\")\n",
        "\n",
        "    def train(self, X, y, batch_size):\n",
        "        history = LossHistory()\n",
        "        sv_mod = callbacks.ModelCheckpoint(self.weights_file, \n",
        "                                           monitor='accuracy', \n",
        "                                           save_best_only=True, \n",
        "                                           save_freq=\"epoch\")\n",
        "        self.model.fit(\n",
        "            X, y, batch_size=batch_size,\n",
        "            epochs=1, verbose=0, callbacks=[history, sv_mod]\n",
        "        )\n",
        "        self.loss_log.append(history.losses)\n",
        "\n",
        "    def log_results(self):\n",
        "        # Save the results to a file so we can graph it later.\n",
        "        with open(self.log_file, 'w') as lf:\n",
        "            wr = csv.writer(lf)\n",
        "            for loss_item in self.loss_log:\n",
        "                wr.writerow(loss_item)\n",
        "        self.loss_log = []\n",
        "\n",
        "    def save_weights_file(self):\n",
        "        if self.weights_file is not None:\n",
        "            self.model.save_weights(self.weights_file, overwrite=True)\n",
        "\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vYa1LAeVi3P"
      },
      "source": [
        "## GenRL- Tod's Generic RL support Library\n",
        "\n",
        "This class allows us to do basic RL stuff.  It does the interfacing with both the Environment simulation (OpenAI's GYM environment) as well as with the Neural Net action predictor.  It also handles data management and training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TvC7sSIN_Mg"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "class Gen_RL(object):\n",
        "\n",
        "    def __init__(self, model, **kwargs):\n",
        "\n",
        "        self.verbose = kwargs.get('verbose', False)\n",
        "        self.num_actions = kwargs.get('num_actions')\n",
        "\n",
        "        self.epsilon = kwargs.get('epsilon', 1)\n",
        "        self.min_epsilon = kwargs.get('min_epsilon', 0.05)\n",
        "        self.batch_size = kwargs.get('batch_size')\n",
        "        self.episodes = kwargs.get('episodes')\n",
        "        self.frames = kwargs.get('frames', 0)\n",
        "        self.replay_size = kwargs.get('replay_size')\n",
        "        self.save_steps = kwargs.get('save_steps', 5000)\n",
        "        self.gamma = kwargs.get('gamma', 0.95)\n",
        "        self.num_inputs = kwargs.get('num_inputs')\n",
        "\n",
        "        self.enable_training = kwargs.get('enable_training', True)\n",
        "\n",
        "        self.replay = []\n",
        "        self.steps = 0\n",
        "\n",
        "        self.model = model\n",
        "        self.counter = 0\n",
        "        \n",
        "        #\n",
        "        # If we send along frames, that replaces episodes as the number of\n",
        "        # steps we're going to take. So we need to reduce epsilon by frame\n",
        "        # in this case, not by \"terminal\" or episode.\n",
        "        #\n",
        "        if self.frames != 0:\n",
        "            self.has_terminal = False\n",
        "            self.epsilon_divider = self.frames\n",
        "        else:\n",
        "            self.has_terminal = True\n",
        "            self.epsilon_divider = self.episodes\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"\"\"\n",
        "                    Creating learner with options:\n",
        "                    Starting epsilon: %d\n",
        "                    Minimum epsilon: %f\n",
        "                    Batch size: %d\n",
        "                    Episodes: %d\n",
        "                    Frames: %d\n",
        "                    Replay (buffer) size: %d\n",
        "                    Gamma: %f\n",
        "                    Actions: %d\n",
        "                    State inputs: %d\n",
        "                \"\"\"\n",
        "                % (self.epsilon,\n",
        "                   self.min_epsilon,\n",
        "                   self.batch_size,\n",
        "                   self.episodes,\n",
        "                   self.frames,\n",
        "                   self.replay_size,\n",
        "                   self.gamma,\n",
        "                   self.num_actions,\n",
        "                   self.num_inputs)\n",
        "            )\n",
        "\n",
        "    def determine_action(self, state):\n",
        "        '''\n",
        "        Here we decide if we should Guess an action (and then learn from the results\n",
        "        of the guess) or if we should predict a value based upon our model.\n",
        "        '''\n",
        "      \n",
        "        #\n",
        "        # Choose an action.  If our random number is lower than the current\n",
        "        # epsilon we will 'guess' an action.\n",
        "        #\n",
        "        if random.random() < self.epsilon and self.enable_training is True:\n",
        "            action = np.random.randint(0, self.num_actions)  # random\n",
        "        else:\n",
        "          \n",
        "            #\n",
        "            # We randomly landed in 'predict' mode, so we need to get Q values \n",
        "            # for each action.  We take the highest ranked action and return.\n",
        "            #\n",
        "            qval = self.model.predict(state)\n",
        "            action = (np.argmax(qval))\n",
        "\n",
        "        return action\n",
        "\n",
        "    def step(self, state, action, reward, new_state, terminal):\n",
        "        '''\n",
        "        Conduct one RL step, train if we have enough data and increment epsilon.\n",
        "        '''\n",
        "       \n",
        "        #\n",
        "        # Store our Experience replay values each step.\n",
        "        #\n",
        "        self.replay.append((state, action, reward, new_state))\n",
        "        \n",
        "        #\n",
        "        # If we've stored enough in our replay buffer, pop off the oldest.\n",
        "        #\n",
        "        if len(self.replay) > self.replay_size:\n",
        "            self.replay.pop(0)\n",
        "            \n",
        "        #\n",
        "        # If we have enough data to make a minibatch, go forth and train.\n",
        "        #\n",
        "        if len(self.replay) > self.batch_size:\n",
        "          \n",
        "            #\n",
        "            # Randomly sample a batch (batch_size) of our experience replay memory\n",
        "            #\n",
        "            minibatch = random.sample(self.replay, self.batch_size)\n",
        "            \n",
        "            #\n",
        "            # Process this minibatch to get a new set of training values.\n",
        "            #\n",
        "            X_train, y_train = self.process_minibatch(minibatch, terminal)\n",
        "            \n",
        "            #\n",
        "            # Train the model on X,y from this minibatch.  This training is\n",
        "            # cumulative, as we load the weights first.\n",
        "            #\n",
        "            self.model.train(X_train, y_train, self.batch_size)\n",
        "            \n",
        "        #\n",
        "        # Decrement epsilon over time. This will determine how often we 'guess'\n",
        "        # vs. how often we 'predict'\n",
        "        #\n",
        "        if self.epsilon > self.min_epsilon:\n",
        "            if (self.has_terminal and terminal) or self.has_terminal is False:\n",
        "                self.epsilon -= (1.0 / (self.epsilon_divider))\n",
        "        \n",
        "                self.epsilon = max (0.05, self.epsilon)\n",
        "          \n",
        "        #   \n",
        "        # Save weights to our model.  NOT USED DUE TO MODELCHECKPOINT callback\n",
        "        #\n",
        "        #if self.model.save_weights and self.model.weights_file is not None:\n",
        "        #    if self.steps % self.save_steps == 0 and self.steps > 0:\n",
        "        #        print(\"Saving weights.\")\n",
        "        #        self.model.save_weights_file()\n",
        "\n",
        "        self.steps += 1\n",
        "        \n",
        "\n",
        "    def process_minibatch(self, minibatch, terminal=False):\n",
        "        ''' \n",
        "        Basic Q-Learning math.  We use our model to predict the current and previous\n",
        "        Q-values for each record in our replay memory batch.\n",
        "        Eventually returns the X and y training arrays.\n",
        "        '''\n",
        "      \n",
        "        X_train = []\n",
        "        y_train = []\n",
        "        \n",
        "        #\n",
        "        # Loop through our batch and create arrays for X and y\n",
        "        # so that we can fit our model at every step.\n",
        "        #\n",
        "        for memory in minibatch:\n",
        "          \n",
        "            #\n",
        "            # Get stored values for each record in the minibatch.\n",
        "            #\n",
        "            old_state_m, action_m, reward_m, new_state_m = memory\n",
        "            \n",
        "            #\n",
        "            # Use trained model to get action prediction on old state.\n",
        "            #\n",
        "            old_qval = self.model.predict(old_state_m)\n",
        "            \n",
        "            #\n",
        "            # Get action prediction on new state.\n",
        "            #\n",
        "            newQ = self.model.predict(new_state_m)\n",
        "            \n",
        "            #\n",
        "            # Estimate the optimal FUTURE value using Numpy's max.\n",
        "            #\n",
        "            maxQ = np.max(newQ)\n",
        "            \n",
        "            #\n",
        "            # Define our label - The previous set of actions\n",
        "            #\n",
        "            y = np.zeros((1, self.num_actions))\n",
        "            y[:] = old_qval[:]\n",
        "            \n",
        "            #\n",
        "            # Check for terminal state (Signifies Episode is complete).\n",
        "            #\n",
        "            if not terminal: # Episode is not complete\n",
        "                #\n",
        "                # Here's where we perform a simple value iteration update, \n",
        "                # using the weighted average of the old action value and the new information\n",
        "                # multiplied by our discount rate (gamma).\n",
        "                #\n",
        "                update = (reward_m + (self.gamma * maxQ))\n",
        "                \n",
        "            else:  # Episode is complete.  No use in applying the future parameters.\n",
        "                update = reward_m\n",
        "                \n",
        "            #\n",
        "            # Assign the new Q-value to y for the action we took.\n",
        "            #\n",
        "            y[0][action_m] = update\n",
        "            \n",
        "            #\n",
        "            # Build training dataset by appending our previous state space\n",
        "            # and our new y labels for previous and future actions.\n",
        "            #\n",
        "            X_train.append(old_state_m.reshape(self.num_inputs,))\n",
        "            y_train.append(y.reshape(self.num_actions,))\n",
        "\n",
        "        X_train = np.array(X_train)\n",
        "        y_train = np.array(y_train)\n",
        "\n",
        "        return X_train, y_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrv8q8QCqfhT"
      },
      "source": [
        "## Pick the Weights File for \"Show\" Mode\n",
        "\n",
        "Right now I have the trained lunarlander file saved in my downloads directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98crw1xCUaJy"
      },
      "source": [
        "!wget -O /content/lunarlander-mod 'https://raw.githubusercontent.com/todnewman/models/master/lunarlander_50_50_50.hdf5'\n",
        "%cd /content\n",
        "%ls -al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOTf3HRB6QnI"
      },
      "source": [
        "## Main Function\n",
        "\n",
        "This section will set up the Neural Net to predict what actions the Lunar Lander should take.  It also governs the initialization of the environment simulation and allows the Lunar Lander to receive environmental state values from the environment simulation and then receive rewards based on the appropriateness of the action.\n",
        "\n",
        "This will train to be able to perform this activity after about 5,000 episodes.\n",
        "\n",
        "To Train the Lander, uncomment the line \"train_or_show = train\" and ensure that \"show\" is commented out.  Once a trained model has been saved, switch to \"show\" mode to load the model into the empty lunar lander and start it predicting actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVGaOA068r3s"
      },
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No9eymbLNsCb"
      },
      "source": [
        "\"\"\"\n",
        "Implements Gen_RL in attempt to solve Gym's Lunar Lander environment.\n",
        "This is the main function that calls the classes in later workbook blocks.\n",
        "Better to execute this block last...\n",
        "\"\"\"\n",
        "\n",
        "import gym\n",
        "import pandas as pd\n",
        "import matplotlib.animation as animation\n",
        "from statistics import mean, stdev\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from google.colab import files\n",
        "from pyvirtualdisplay import Display\n",
        "from tensorflow.keras import callbacks\n",
        "import logging\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "\n",
        "import psutil\n",
        "%matplotlib inline\n",
        "\n",
        "#\n",
        "# Set up the display here to work inside CoLaboratory.  This will enable the\n",
        "# videos at the end. When we're training large data sets, should probably comment\n",
        "# this out because it uses lots of memory!\n",
        "#\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "frames = []\n",
        "\n",
        "#\n",
        "# Change the number of episodes.  About 5K episodes are needed to train the \n",
        "# model effectively.\n",
        "#\n",
        "episodes =  40\n",
        "\n",
        "#\n",
        "# The below won't change for LunarLander\n",
        "#\n",
        "inputs = 8\n",
        "actions = 4\n",
        "\n",
        "#\n",
        "# Just change these to switch from training mode to inference (show) mode.\n",
        "# When in 'show' mode you need to make sure you have a weights file uploaded\n",
        "# (see previous block)\n",
        "#\n",
        "\n",
        "#train_or_show = 'train'            # TRAINING the agent\n",
        "train_or_show = 'show'            # LOAD Trained Model in an Empty Agent\n",
        "\n",
        "\n",
        "log_file = 'loss_log.txt'\n",
        "\n",
        "#\n",
        "# Define parameters for training vs. inference\n",
        "#\n",
        "if train_or_show == 'train':\n",
        "    weights_file = 'lunarlander-train'\n",
        "    enable_training = True\n",
        "    load_weights = False\n",
        "    save_weights = True\n",
        "else:\n",
        "    weights_file = 'lunarlander-mod'\n",
        "    enable_training = False\n",
        "    load_weights = True\n",
        "    save_weights = False\n",
        "#\n",
        "# Create the environment. You can change this to other Gym environments\n",
        "# to experiment.  Here we'll print out the action and observation spaces.\n",
        "#\n",
        "env = gym.make('LunarLander-v2')\n",
        "print(\"action space: {0!r}\".format(env.action_space))\n",
        "print(\"observation space: {0!r}\".format(env.observation_space))\n",
        "\n",
        "obs_shape = env.observation_space.shape\n",
        "\n",
        "#\n",
        "# Set up the network by creating an instance of the GenNet class.\n",
        "#\n",
        "network = GenNet(nodes_1=50, nodes_2=50, nodes_3=50, num_actions=actions, \n",
        "                   num_inputs=inputs, verbose=True,\n",
        "                   load_weights=load_weights, weights_file=weights_file, \n",
        "                   obs_shape=obs_shape, save_weights=save_weights, \n",
        "                   log_file=log_file)\n",
        "                  \n",
        "                 \n",
        "#\n",
        "# Setup Gen RL's deep RL model by creating an instance of the Gen_RL class.\n",
        "#\n",
        "rl = Gen_RL(network, episodes=episodes, num_actions=actions,\n",
        "                  batch_size=32, min_epsilon=0.05, num_inputs=inputs,\n",
        "                  replay_size=10000, gamma=0.95, verbose=True,\n",
        "                  enable_training=enable_training)\n",
        "\n",
        "def show_RAM_usage():\n",
        "    py = psutil.Process(os.getpid())\n",
        "    print('RAM usage: {} GB'.format(py.memory_info()[0]/2. ** 30))\n",
        "\n",
        "rewards = []\n",
        "results = []\n",
        "max_steps = 600\n",
        "repeat_action = 3\n",
        "\n",
        "\n",
        "# Run.\n",
        "for i in range(episodes):\n",
        "    K.clear_session() # for the TF2.0 Memory Leak?\n",
        "    start = time.time()\n",
        "\n",
        "    # Get initial state.\n",
        "    state = env.reset()\n",
        "\n",
        "    terminal = False\n",
        "    e_rewards = 0\n",
        "    steps = 0\n",
        "\n",
        "    while not terminal:\n",
        "        steps += 1\n",
        "        \n",
        "        #\n",
        "        # Send the state to Gen_RL, get our predicted action.\n",
        "        #\n",
        "        action = rl.determine_action(state)\n",
        "        #\n",
        "        # Best results come from performing the same action multiple times.  This\n",
        "        # comes from the original DeepMind DQN paper.  This improves training time\n",
        "        # but doesn't really sacrifice performance.  \n",
        "        # Each time, the environment (OpenAI Gym) gives us the new state, a reward,\n",
        "        # and whether the Episode is terminal or not.\n",
        "        #\n",
        "        for x in range(repeat_action):\n",
        "            new_state, reward, terminal, _ = env.step(action)   \n",
        "        #\n",
        "        # Create an array of frames from the environment's 'render' feature.\n",
        "        # This is primarily done in CoLaboratory due to the difficulty of\n",
        "        # rendering to a display.  Outside of CoLab I'd just call env.render().\n",
        "        # We switch this out when training large datasets because it will use all your\n",
        "        # memory.\n",
        "        #\n",
        "        if train_or_show == 'show' or (train_or_show == 'train' and episodes < 60):\n",
        "            frames.append(env.render(mode = 'rgb_array'))\n",
        "        #\n",
        "        # Add the info to our experience replay for training.  Experience Replay\n",
        "        # is the breakthrough where data acquired during the online learning\n",
        "        # process are stored and presented repeatedly to the underlying\n",
        "        # RL algorithm. This increases data efficiency, while exploiting\n",
        "        # the computational efficiency of the underlying algorithm.  There is a\n",
        "        # lot of research into ER these days to improve it and mitigate its liabilities.\n",
        "        # - https://arxiv.org/abs/1712.01275\n",
        "        # - http://www.busoniu.net/files/papers/smcc11.pdf\n",
        "        #\n",
        "        if enable_training:\n",
        "            rl.step(state, action, reward, new_state, terminal)   \n",
        "        #\n",
        "        # Accumulate rewards.\n",
        "        #\n",
        "        e_rewards += reward\n",
        "        \n",
        "        #\n",
        "        # If the Episode is complete.\n",
        "        #\n",
        "        if terminal:\n",
        "            \n",
        "            #\n",
        "            # The Episode is complete.  If we aren't training, be more verbose \n",
        "            # about the mission.\n",
        "            #\n",
        "            if not enable_training:\n",
        "                if reward == -100:\n",
        "                    result = 'Crashed'\n",
        "                else:\n",
        "                    result = 'Landed'\n",
        "                print(\"%s! Score: %d\" % (result, e_rewards))\n",
        "\n",
        "            # Create Rewards Array.\n",
        "            rewards.append(e_rewards)\n",
        "            if len(rewards) > 10:\n",
        "                rewards.pop(0)\n",
        "            e_rewards = 0\n",
        "            \n",
        "        #\n",
        "        # Update state for the next iteration in this episode. Our next action\n",
        "        # will be based off of this state.\n",
        "        #\n",
        "        state = new_state\n",
        "        \n",
        "        #\n",
        "        # This keeps us from getting stuck.\n",
        "        #\n",
        "        if steps > max_steps:\n",
        "            print(\"Too many steps.\")\n",
        "            break\n",
        "        #end = time.time()\n",
        "        #print(end - start)\n",
        "    #\n",
        "    # Every 10th episode, print out parametrics.  This will help us determine\n",
        "    # how well the training is progressing.\n",
        "    #\n",
        "    if i % 10 == 0 and i > 0:\n",
        "        end = time.time()\n",
        "        show_RAM_usage()\n",
        "        print(\"-\"*80)\n",
        "        print(\"Epsilon: %.5f\" % rl.epsilon)\n",
        "        print(\"Episode: %d\" % i)\n",
        "        print(\"Mean: %.2f\\tMax: %d\\tStdev: %.2f\" %\n",
        "              (mean(rewards), max(rewards), stdev(rewards)))\n",
        "        print(f\"Time for 10 Episodes: {(end-start)*10}\")\n",
        "        results.append(mean(rewards))\n",
        "\n",
        "# Just print out all of our mean rewards so we can plot them or\n",
        "# do other fun things.\n",
        "\n",
        "#plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "\n",
        "\n",
        "if train_or_show == 'show':\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    animate = lambda i: patch.set_data(frames[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "else:\n",
        "    patch = plt.imshow(frames[0])\n",
        "    animate = lambda i: patch.set_data(frames[i])\n",
        "    network.log_results()\n",
        "    df3 = pd.read_csv(log_file, header=0)\n",
        "    df3.columns = ['Loss']\n",
        "    df3.plot(y='Loss')\n",
        "    for r in results:\n",
        "        print(r)\n",
        "\n",
        "env.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK2ZtjedKw21"
      },
      "source": [
        "## Create JSHTML video\n",
        "\n",
        "Rendering Javascript HTML so you can see the performance of the lander.  Be patient, this takes just over a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gU6gcPAql1jr"
      },
      "source": [
        "%time HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNmdk7vmKo7O"
      },
      "source": [
        "## Create HTML5 (MP4) video\n",
        "Here we can create a MP4 movie that can be sent as an attachment, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUjPkuUWAOrk"
      },
      "source": [
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!apt install ffmpeg\n",
        "\n",
        "\n",
        "patch = plt.imshow(frames[0])\n",
        "animate = lambda i: patch.set_data(frames[i])        \n",
        "ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "%time HTML(ani.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcO4XHY5KYRJ"
      },
      "source": [
        "## Save Video to Downloads folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orKKsKTrvpkh"
      },
      "source": [
        "\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "\n",
        "\n",
        "Writer = animation.writers['ffmpeg']\n",
        "writer = Writer(fps=15, metadata=dict(artist='TodN'), bitrate=1800)\n",
        "ani.save('working_inference.mp4', writer=writer)\n",
        "files.download('working_inference.mp4')  # from colab to browser download\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l88jCD-sWhrL"
      },
      "source": [
        "## Google Drive authentication Stuff\n",
        "\n",
        "I don't like this much, but right now this is how I can allow the RL Models to be saved to the Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo5uXqI-lI_r"
      },
      "source": [
        "print(piglet.__version__)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lYnHPDflNJq"
      },
      "source": [
        "#!cp /content/lunarlander drive/.\n",
        "%cd lunarlander/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XLDIPQIwYy4"
      },
      "source": [
        "%cd /content\n",
        "%ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjbbEBrNwg7u"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('loss_log.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmLBaGjnxjSX"
      },
      "source": [
        "%cd lunarlander/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAyWp0vUyBQm"
      },
      "source": [
        "files.download('lunarlander.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89ZxlSOEyKdG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}